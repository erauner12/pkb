#+SETUPFILE: ../../styles/readtheorg.setup
#+TITLE: Kubernetes
#+HTML_HEAD: <script> var HS_STARTUP_FOLDED = true; </script>

* Links

https://app.pluralsight.com/library/courses/getting-started-kubernetes/table-of-contents

* Vagrant Lab

https://blog.exxactcorp.com/building-a-kubernetes-cluster-using-vagrant/


#+BEGIN_SRC go
git clone https://exxsyseng@bitbucket.org/exxsyseng/k8s_centos.git      # Centos k8s Cluster
git clone https://exxsyseng@bitbucket.org/exxsyseng/k8s_ubuntu.git      # Ubuntu k8s Cluster
#+END_SRC



* Course Overview

** Course Introduction

fundamental parts of kubernetes

- going to install kubernetes for a lab

- going to work with pods

- going to do some deployments



* Course Introduction

** Course intro

* What Is Kubernetes?

** Kubernetes: Where It Came From

- written in go/golang
  - https://github.com/kubernetes/kubernetes

- came from google
- borg
  - omega
    - kubernetes


Kubernetes = K8s



** Kubernetes: What and Why

Containers
- Challenges they bring
  - brings new scalability challenges

  - application developers do not care about cpu is running their code

  - Pets vs Cattle

    - kubernetes, I got this app and it consists of these containers

      - kubernetes take care of all the hard work and logistics


- what is it made up of?

  - standard package format

  - manifest


- very platform agnostic

  - as long as you can install the agent, it wil work


- lets you target deployments

  - it makes decisions about to where to run it

- Kubernetes is moving very fast

  - get your hands dirty and keep playing with it

* Kubernetes Architecture

** Module Intro

Big picture view

- masters

- nodes

- pods[[

- Services

  - networking

- deployments

** Big Picture View
    
Kubernetes is just an orchestrator for microservice apps

file:../../images/kubernetes.org_20200415_193424_e4YYOS.png

package up your application and give it to the cluster

file:../../images/kubernetes.org_20200415_193726_LpcgYX.png




- Masters

  - master control plane

  - 

- Nodes

  - do the actual work
  - report back to master about the changes that are happening


- package the application it up in what is called a deployment

  - manifest file tells cluster what configuration settings it has





** Masters

platform agnostic
file:../../images/kubernetes.org_20200415_194008_jZjkb2.png
- all it wants is linux
  - linux does not care if it running on bare metal,openstack, cloud


kube-apiserver
- front-end to the control plane
- exposes the API (REST)
- Consumes JSON via manfiest files
  - declare the state of our app for. record of intent


- Cluster Store

  - persistent storage
  - cluster state and con fig
  - uses etcd
    - source of truth for the cluster


- kube controller-manager

  - controller of controllers

  - node controller

  - endpoints controller

  - namespace controller

  - watches for changes

  - helps maintain desired state
    - current state = desired state

- kuber-scheduler

  - watches apiserver for new pods

  - assigns work to nodes

file:../../images/kubernetes.org_20200415_194815_YRQj8A.png


- issuing commands to the master
  - master = api server
    - that is where you are actually issuing commands

- kubectl talks to the apiserver (master) via json files
  - works gets passed to nodes from the scheduler

file:../../images/kubernetes.org_20200415_195033_dNRDgH.png

** Nodes

- kubelet
 
  - kubelet is the main kubernetes agent on the node (pretty much is the node)

    - talks to containers

    - registers node with cluster
    - watches apiserver
    - instantiates pods
    - then it reports back the state back to the master
      - kubelet itself does not do anything if something goes wrong, just reports it

    - a pod is just a bunch of containers packaged together as a single unit

      - because it is interacting with a container

  - kubelet needs to interact container runtime
    - container runtime examples (pluggable) does not matter which
      - docker
      - rkt


- kube proxy

  - network brains of the node

  - makes sure every pod gets it's ip

    - ip per pod

    - if you want to interface with each container in the pod, you will need to interface with a port

  - proxy is also load balances across all pods in a service

  - bunch of web servers talk to a backend

    - a service will load balance the traffic from a web server pod to a backend pod

file:../../images/kubernetes.org_20200415_202902_rwb6NU.png


wrapup
- kubelet
- container engine
- kube-proxy


** Desired State and the Declarative Model

- kubernetes operates on declarative model

  - YAML or JSON is contains the desired state

    - they are manifest files that describe a record of intent 


    - it's up to kubernetes to get it there at that point
      - pulling images
      - building networks
      - starting containers


file:../../images/kubernetes.org_20200415_203208_jDExm4.png


- example of desired state of a cluster

  -  kubernetes control plane runs through reconciliation loops that constantly check if desired state = actual state


  - 3 nodes, one pod is running on each of those nodes:

file:../../images/kubernetes.org_20200415_203611_C3Fuu0.png

- node goes down

file:../../images/kubernetes.org_20200415_203648_ajXxvQ.png

** Pods

- vmware runs vms
- docker runs containers

- kubernetes runs pods
  - cannot run a container directly on kubernetes
  - needs to be running on a pod

  - typically one container running on a pod but in more advanced configuration you can have more than one container running on a pod


- what actually is a pod

  - ring-fenced environment

  - file:../../images/kubernetes.org_20200415_204247_XeWH4M.png
  - if the use case for containers are tightly coupled and need access to the same resources

    - stick them in the same pod

    - file:../../images/kubernetes.org_20200415_204413_q0l134.png

    - if the use case can be loosely coupled, stick them in separate pods in couple them over the networks

    - file:../../images/kubernetes.org_20200415_204539_t4x81j.png

    - the unit of scaling in kube is the pod

      - you want to scale an app? you would add/remove pods

      - you don't scale by adding more containers to an existing pod
	- add/remove pod replicas

      - file:../../images/kubernetes.org_20200415_204709_iAeHa3.png

  - example of a multi-container pod

    - two or more complimentary containers
      - web server and log scraper tailing logs off somewhere else

      - file:../../images/kubernetes.org_20200415_205121_fc31Rc.png

      - file:../../images/kubernetes.org_20200415_205155_iwHUeu.png

	- web server = main
	- log scraper = sidecar

- pods are atomic

  - it is either not there or there
    - no state that it is in the process of coming up
    - pod is not declared available until the whole pod is up and running

  - you cannot have a single pod spread over multiple nodes

  - pods's life cycle

    - they are born, live and die. you don't bring one back to life

    - file:../../images/kubernetes.org_20200415_205509_7BGlRp.png

    - this is consistent with pet vs cattle. when pods die, a totally new one can pop up in it's place and replace it

  - pod deployments

    - replace replication controllers

** Services

- pods die and come back up somewhere else in the cluster (on a different node possibly)

  - example of how IP address churn is addressed

  - say pods running apps need access to pods running databases
    - ips changing constantly. lose and gain when you scale

  - this is where services come to play

  - provides a single dns name and IP here so that the pods IP have nothing to do with it
  - file:../../images/kubernetes.org_20200415_210400_X7Jizk.png

  - if a pod dies and replaces itself, service updates and knows about it

  - example of this happening is when the pods are being updated to a new version, can auto scale from 2 to 4 and back to two as easy as that

- labels

  - labeled BE pods with relevant information, the labels tie the pod to the service

  - file:../../images/kubernetes.org_20200415_210806_yR1gZf.png

  - when we are updating can remove the tag on the service that makes it exclusive to servicing one version

  - file:../../images/kubernetes.org_20200415_210929_ZDY4Ij.png

    - load balancing across them all until we are ready to be fully on the new version

  - file:../../images/kubernetes.org_20200415_211024_HEbPA3.png

  - services only send traffic to healthy pods

  - can configure the service to point to things outside the cluster

** Deployments

- our infrastructure = masters and nodes
  - file:../../images/kubernetes.org_20200415_211946_63N0dm.png

  - pods are running on the nodes

- deployments = declarativeness

  - declare through the manfiest yml or json files

  - self documenting, versioned, spec-once deploy many

  - makes for much simpler rollbacks and rolling updates

  - the declaration is deployed by kube via the apiserver

- rolling updates

  - can run multiple concurrent versions

  - blue-green deployments
  - canary releases

** Bringing It Home

- just give us linux, we'll give you kubernetes
- master node setup

- master (control plane
  - apiserver
  - cluster store
    - api server talks to it

- node/minion

  - where the work happens

  - kubelet
  - container engine
  - kube-proxy


- example

  - issue work to master via api server (instruction = manifest files)

  - master decides where to run the work

    - gets dished out to the worker nodes

    - they report back state changes to the master


- rest objects in kubernets api
  - pods
    - atomic unit of scheduling
  - replication controllers
    - old

  - deployments
    - RC, rolling updates, rollbacks

  - services
    - stable networking

* Installing Kubernetes

** Module Intro

going to use minikube
- cerner dailyprogrammer uses kind

** Minikube

*** mac

#+CAPTION:
#+BEGIN_SRC go
brew install kubectl
kubectl --version
#+END_SRC

#+CAPTION:
#+BEGIN_SRC go
brew cask install minikube
#+END_SRC

*** windows
https://technology.amis.nl/2019/02/12/rapidly-spinning-up-a-vm-with-ubuntu-docker-and-minikube-using-the-vm-drivernone-option-on-my-windows-laptop-using-vagrant-and-oracle-virtualbox/

go straight to here:

Please click for: Vagrantfile
Please click for: docker.sh
Please click for: minikube.sh
Please click for: kubectl.sh


#+CAPTION:
#+BEGIN_SRC go
vagrant up
#+END_SRC

http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/http:kubernetes-dashboard:/proxy/

** Google Container Engine
n/a
** Installing in AWS with kops
n/a
** Installing Manually with kubeadm

n/a
** Module Summary


- smallest unit of scheduling is a pod (atomic)

- pod manfests (yaml or JSON)

- deployed via other objects
  - replication controllers etc

- replication controllers
  - implement desired state


* Working with Pods


** Pod Theory

- VM = Container in the docker world
  - kubernetes = pods are the atomic unit of scheduling

file:../../images/kubernetes.org_20200418_181935_M8nSpF.png

- deploy a pod to a cluster by giving it a manifest file
  - api server deploys it to a node


file:../../images/kubernetes.org_20200418_182037_nDoMNC.png

- pod gets one IP
  - one IP for each pod
  - access each container by the port
    - each ip is it's own network namespace
    - each container needs to be accessed on it's own port. two containers cannot share the same port

file:../../images/kubernetes.org_20200418_182316_Pvj4kb.png

- interpod communication
  - every pod talk to every other pod
file:../../images/kubernetes.org_20200418_182400_5rUGA9.png


- intra-pod communication (within the pod)

file:../../images/kubernetes.org_20200418_182454_73XXGe.png


- no such thing as a half deployed
  - pod cannot be parcially up

- one pod : one node
  - cannot have a pod on two nodes

- you can more than oen container on one pod though

- scheduled on nodes (monions)

- pod lifecycle
  - manifest -> api server
    - pod pending state
      - pod -> running state
	- pod either succeeds
	- pod failed

file:../../images/kubernetes.org_20200418_182719_KCcaeg.png







** Deploying Your First Pod

#+CAPTION:pod.yml
#+BEGIN_SRC go
apiVersion: v1
kind: Pod
metadata:
  name: hello-pod
spec:
  containers:
  - name: hello-ctr
    image: nigelpoulton/pluralsight-docker-cilatest
    ports:
    - containerPort: 8080
#+END_SRC

- setting the api version
- setting what kind of object to deploy
- setting the name of the pod
- spec is setting what is in the resource
  - naming the image and specifying the imae
  - if it is going to be a multi-container pod would just set more up down below this section


#+CAPTION:creating the pod
#+BEGIN_SRC shell
vagrant@ubuntu-xenial:~$ kubectl create -f pod.yml
pod/hello-pod created


vagrant@ubuntu-xenial:~$ kubectl get pods
NAME        READY   STATUS              RESTARTS   AGE
hello-pod   0/1     ContainerCreating   0          2s

vagrant@ubuntu-xenial:~$ kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
hello-pod   1/1     Running   0          25s
#+END_SRC

#+CAPTION:filter when getting pods
#+BEGIN_SRC shell
kubectl get pods/hello-pod
#+END_SRC


#+CAPTION:get all pods in all namespaces
#+BEGIN_SRC shell
kubetl get pods --all-namespaces
#+END_SRC

- we do not typicall manage pods like this though

- assume you wanted 5 instances of a pod, and wanted the desired state to be that way
file:../../images/kubernetes.org_20200418_184703_zV5rBP.png

- kubernetes makes sure it always stays that way






** Deploying Pods via Replication Controllers
#+CAPTION:rc.yml
#+BEGIN_SRC yml
apiVersion: v1
kind: ReplicationController
metadata:
  name: hello-rc
spec:
  replicas: 10
  selector:
    app: hello-world
  template:
    metadata:
      labers:
        app: hello-world
  containers:
    - name:
      image: nigelpoulton/pluralsight-docker-ci:latest
      ports:
      - containerPort: 8080
#+END_SRC

#+CAPTION:create what you just made
#+BEGIN_SRC go
kubectl create -f rc.yml
#+END_SRC

#+CAPTION:upp the amount of instances
#+BEGIN_SRC yml
<change rc.yml>

vagrant@ubuntu-xenial:~$ kubectl apply -f rc.yml
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
replicationcontroller/hello-rc configured
#+END_SRC

#+CAPTION: see that it is 20 now
#+BEGIN_SRC go
vagrant@ubuntu-xenial:~$ kubectl get pods              
NAME             READY   STATUS    RESTARTS   AGE      
hello-rc-2tqmt   1/1     Running   0          5m36s    
hello-rc-54q76   1/1     Running   0          5m36s    
hello-rc-7bpdg   1/1     Running   0          5m36s    
hello-rc-7kk4c   1/1     Running   0          7m28s    
hello-rc-8kmkc   1/1     Running   0          5m36s    
hello-rc-cj99d   1/1     Running   0          7m28s    
hello-rc-fbffc   1/1     Running   0          7m28s    
hello-rc-jbjlz   1/1     Running   0          5m36s    
hello-rc-jnxc5   1/1     Running   0          5m36s    
hello-rc-k59v6   1/1     Running   0          7m28s    
hello-rc-k5grr   1/1     Running   0          7m28s    
hello-rc-l4nst   1/1     Running   0          7m28s    
hello-rc-mlsrl   1/1     Running   0          5m36s    
hello-rc-pf9s8   1/1     Running   0          5m36s    
hello-rc-qd5zp   1/1     Running   0          7m28s    
hello-rc-rb262   1/1     Running   0          5m36s    
hello-rc-rhfhv   1/1     Running   0          7m28s    
hello-rc-vzvzr   1/1     Running   0          7m28s    
hello-rc-x72kl   1/1     Running   0          5m36s    
hello-rc-zksnl   1/1     Running   0          7m28s    
#+END_SRC

** Module Summary

- theory of services

- create a service the iterative way

- create a serice the decalairtive way

- real-world application

- recap
** Kubernetes Services

- services are rest objects in the K8s API
  - services are an abstraction
  - services = reliable
    - pods are not

- we have no solid way of connecting to the pods right now
  - file:../../images/kubernetes.org_20200418_190633_51bw9l.png

    - service gets it's own IP in the middle that never changes

  - no matter what is changing on the replication controller, the service accomodates it


- you can hit any of the nodes in the cluster on the port that the container is running on:

  - file:../../images/kubernetes.org_20200418_191019_cZheVp.png

  - whatever comes in or leaves, the service keeps an endpoint directory


- labels

file:../../images/kubernetes.org_20200418_191202_o29yew.png

- tell the service to look at the label of the pods
  - version label specifically


- service discovery

  - DNS based (best)
    - makes sure kubeletes (nodes) can be found

** The Theory


** Creating a Service the Iterative Way

- going to create a service

#+BEGIN_SRC go
vagrant@ubuntu-xenial:~$ kubectl expose rc hello-rc --name=hello-svc  --target-port=8080 --type=NodePort
service/hello-svc exposed


vagrant@ubuntu-xenial:~$ kubectl describe svc hello-svc
Name:                     hello-svc
Namespace:                default
Labels:                   app=hello-world
Annotations:              <none>
Selector:                 app=hello-world
Type:                     NodePort
IP:                       10.102.123.13
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  31894/TCP
Endpoints:                172.17.0.10:8080,172.17.0.11:8080,172.17.0.12:8080 + 17 more...
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

#+END_SRC





** Creating a Service the Declarative Way


** In the Real World


** Summary


* Kubernetes Deployments


** Module Intro


** Deployment Theory


** Creating Your First Kubernetes Deployment


** Updating a Deployment


** Module Summary


* What Next?


** What Next?
file:../../images/kubernetes.org_20200418_143658_tLYQXO.png
